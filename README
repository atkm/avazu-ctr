train has 40M lines.
test has 4M.

TODO:
- Submit Model 3
- use device_ip instead of device_id to model users.
    Re-run Model 2 and 3.
- Model 4
- Correlation of features
- hour: morning, afternoon, night; weekend or not.
- high-cardinality anonymous variables: C14, C17, C20 (Model 1.5).
- Parameters of the lg model to tune:
    + elasticNetParam & regParam: L2 by default. Set elasticNetParam=1 to use L1.
    + standardization: categorical variables shouldn't need standardization.
    + weightCol: correct for inbalance.

Model 4:
- both user-site and user-category interactions.

Model 3:
- user-site interaction (site_id, device_ip), (app_id, device_ip)

Model 2:
- add user-category interaction: count per day (site_category, device_ip), (app_category, device_ip)

Mode 1.5:
- add C14, C17, and C20 to the model.

Model 1 (basemodel):
- C1
- banner_pos
- site_category_vec
- app_category_vec
- device_type
- device_conn_type
- C15
- C16
- C18
- C19
- C21

No idea yet:
- hour
- device_model
- high-cardinality anonymous variables: C14, C17, C20

Not using:
- site_domain, app_domain: correlated with site/app_id (check)
- device_id: correlated with device_ip (check)

Ideas:
- Does device_id roughly model users?
    It is most likely noisy, since unique users aren't easy to model.
- LogisticRegression, NaiveBayes, Tree ensemble (use probabilityCol) (slow?)
- An user is more likely to click if she clicked previously.
    The number of unique (device_ip, id) tuples is similar to that of device_ip's, so use ip's to represent users.
- Ads on a site and those on an app are mutually exclusive.
    Create separate models for the two types of ads.
- To reduce the number of features when encoding a categorical variable, use count, ranking by count, or probability of each category.
    Other ways to deal with the same issue: feature hashing, embedding into a vector space (embedding_column in tf).
    Look into "locally sensitive hashing" algorithms in Spark.
- Feature selection: sklearn's SelectKBest (and its family), L1, tree models.
    Spark doesn't do recursive elimination, but does chi-square test.
- How to find correlated categorical variables?
- Metrics.
    Use log-loss to evaluate. Others: precision, recall, F2, accuracy (ratio of correct).
- How to model "if the user clicked on an ad previously, she is likely to click again"?
    Use of device_ip and device_id.

Facts:
- The click rate for train_small.csv is 0.170. 0.175 for the full data set.
- In the test set, site_id and app_id are mutually exclusive.
- Spark handles string categoricals better than sklearn.
    sklearn's LabelEncoder doesn't equal Spark's StringIndexer.
    CategoricalEncoder is coming to sklearn soon (maybe?)

Ref:
- Winner (also the winner of the criteo comp)
    https://github.com/guestwalk/kaggle-avazu
- 2nd place soln  https://github.com/owenzhang/kaggle-avazu
- Feature ideas: https://medium.com/unstructured/how-feature-engineering-can-help-you-do-well-in-a-kaggle-competition-part-i-9cc9a883514d
